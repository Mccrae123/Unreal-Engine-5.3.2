// Copyright Epic Games, Inc. All Rights Reserved.

// TODO: should this be 1 by default so compute always has access to it?
#ifndef USES_PER_INSTANCE_RANDOM
	#define USES_PER_INSTANCE_RANDOM 1
#endif

#include "../Common.ush"
#include "../SceneData.ush"
#include "../LightmapData.ush"
#include "InstanceCullingCommon.ush"
#include "../Nanite/NaniteDataDecode.ush"
#include "../Nanite/HZBCull.ush"
#include "../WaveOpUtil.ush"
#include "../ComputeShaderUtils.ush"

#if SINGLE_INSTANCE_MODE
	// Enable a load balancer optimization where all items are expected to have a single instance
	#define LOAD_BALANCER_SINGLE_INSTANCE_MODE 1
#endif

#include "InstanceCullingLoadBalancer.ush"

uint InstanceSceneDataSOAStride;
uint GPUSceneNumInstances;
uint GPUSceneNumPrimitives;
uint GPUSceneNumLightmapDataItems;

#ifndef BATCH_PROCESSING_MODE_NUM
	#define BATCH_PROCESSING_MODE_NUM 1
#endif // BATCH_PROCESSING_MODE_NUM

StructuredBuffer<FDrawCommandDesc> DrawCommandDescs;
Buffer<uint> InstanceIdOffsetBuffer;
StructuredBuffer<uint> ViewIds;

uint NumViewIds;
uint DynamicInstanceIdOffset;
uint DynamicInstanceIdMax;
uint NumCullingViews;
uint CurrentBatchProcessingMode;

struct FContextBatchInfo
{
	uint IndirectArgOffset;
	uint InstanceDataWriteOffset;
	uint ViewIdsOffset;
	uint NumViewIds;
	uint DynamicInstanceIdOffset;
	uint DynamicInstanceIdMax;
	uint ItemDataOffset[BATCH_PROCESSING_MODE_NUM];
};


#if ENABLE_BATCH_MODE
StructuredBuffer<FContextBatchInfo> BatchInfos;
StructuredBuffer<uint> BatchInds;
#endif // ENABLE_BATCH_MODE

#if DEBUG_MODE
int bDrawOnlyVSMInvalidatingGeometry;
#endif // DEBUG_MODE


RWStructuredBuffer<uint> InstanceIdsBufferOut;
RWStructuredBuffer<float4> InstanceIdsBufferOutMobile;
RWBuffer<uint> DrawIndirectArgsBufferOut;

// this is just to avoid compiling packing code for unrelated shaders
#define USES_PACKED_INSTANCE_DATA (FEATURE_LEVEL == FEATURE_LEVEL_ES3_1 && VF_SUPPORTS_PRIMITIVE_SCENE_DATA)

#if USES_PACKED_INSTANCE_DATA

float4 PackLightmapData(float4 LightmapUVScaleBias, float4 ShadowmapUVScaleBias)
{
	float4 Ret;
	Ret.x = asfloat(PackUnorm2x16(LightmapUVScaleBias.xy));
	Ret.y = asfloat(PackSnorm2x16(LightmapUVScaleBias.zw));
	Ret.z = asfloat(PackUnorm2x16(ShadowmapUVScaleBias.xy));
	Ret.w = asfloat(PackSnorm2x16(ShadowmapUVScaleBias.zw));
	return Ret;
}

void WritePackedInstanceData(uint Offset, uint InstanceId, FInstanceSceneData InstanceData, uint ViewIdIndex)
{
	FPrimitiveSceneData PrimitiveData = GetPrimitiveData(InstanceData.PrimitiveId);
	
	float4 LightMapUVScaleBias = float4(1,1,0,0);
	float4 ShadowMapUVScaleBias = float4(1,1,0,0);

	// FIXME: need LODIndex to compute correct LightmapDataIndex
	uint LightmapDataIndex = PrimitiveData.LightmapDataIndex; // + LODIndex;
	if (LightmapDataIndex < GPUSceneNumLightmapDataItems)
	{
		LightMapUVScaleBias = GetLightmapData(LightmapDataIndex).LightMapCoordinateScaleBias;
		ShadowMapUVScaleBias = GetLightmapData(LightmapDataIndex).ShadowMapCoordinateScaleBias;
		if (InstanceData.HasLightShadowUVBias)
		{
			LightMapUVScaleBias.zw = InstanceData.LightMapAndShadowMapUVBias.xy;
			ShadowMapUVScaleBias.zw = InstanceData.LightMapAndShadowMapUVBias.zw;
		}
	}
	else
	{
		// avoid passing invalid lightmap data index into gfx shaders?
		LightmapDataIndex = 0;
	}

	float4 PackedLightmapData = PackLightmapData(LightMapUVScaleBias, ShadowMapUVScaleBias);

	const uint PackedPrimitiveFlags = ((PrimitiveData.Flags & 0xffff) | (InstanceData.Flags << 16u));

	float4 InstanceOriginAndId = InstanceData.LocalToWorld[3];
	InstanceOriginAndId.w = asfloat(InstanceData.PrimitiveId);

	float4 InstanceTransform1 = InstanceData.LocalToWorld[0];
	float4 InstanceTransform2 = InstanceData.LocalToWorld[1];
	float4 InstanceTransform3 = InstanceData.LocalToWorld[2];
	
	InstanceTransform1.w = asfloat(PackedPrimitiveFlags); 
	InstanceTransform2.w = asfloat(LightmapDataIndex); // TODO: pack something useful here 
	InstanceTransform3.w = InstanceData.PerInstanceRandom;
		
	InstanceIdsBufferOutMobile[Offset*5 + 0] = InstanceOriginAndId;
	InstanceIdsBufferOutMobile[Offset*5 + 1] = InstanceTransform1;
	InstanceIdsBufferOutMobile[Offset*5 + 2] = InstanceTransform2;
	InstanceIdsBufferOutMobile[Offset*5 + 3] = InstanceTransform3;
	InstanceIdsBufferOutMobile[Offset*5 + 4] = PackedLightmapData;
}
#endif

void WriteInstance(uint Offset, uint InstanceId, FInstanceSceneData InstanceData, uint ViewIdIndex)
{
	checkSlow(InstanceId < GPUSceneNumInstances);

#if USES_PACKED_INSTANCE_DATA
	WritePackedInstanceData(Offset, InstanceId, InstanceData, ViewIdIndex);
#else
	uint PackedId = InstanceId | (ViewIdIndex << 28U);
	checkStructuredBufferAccessSlow(InstanceIdsBufferOut, Offset);
	InstanceIdsBufferOut[Offset] = PackedId;
#endif
}

bool ShouldRenderInstance(FInstanceSceneData InstanceData, FDrawCommandDesc Desc)
{
#if DEBUG_MODE
	BRANCH
		if (bDrawOnlyVSMInvalidatingGeometry != 0)
		{
			const bool bHasMoved = GetGPUSceneFrameNumber() == InstanceData.LastUpdateSceneFrameNumber || Desc.bMaterialMayModifyPosition;

			// TODO: Fix the shadow flag here:
			const bool bCastShadow = (GetPrimitiveData(InstanceData.PrimitiveId).Flags & 1u) != 0u;

			return bHasMoved && bCastShadow;
		}
#endif // DEBUG_MODE
	return true;
}

bool IsInstanceVisible(FInstanceSceneData InstanceData, uint ViewIdIndex)
{
#if CULL_INSTANCES

	// TODO: The test for dot(InstanceData.LocalBoundsExtent, InstanceData.LocalBoundsExtent) <= 0.0f is just a workaround since the FDynamicMeshBuilder::GetMesh
	//       seems to just set empty bounds (and FLineBatcherSceneProxy pretends everything is at the origin). In the future these should compute reasonable bounds and 
	//       this should be removed.
	// NOTE: Flagging invalid as visible. This is a workaround that prevents instances that are uploaded after this kernel runs from being culled. 
	// When the culling tests are moved into the later kernel we should switch this back to discarding. Batching moved them back up the pipeline so here we are.
	// This happens for dynamic instances (e.g., procedural mesh) since they are uploaded with the view after the deferred/batched culling. To solve this we could
	// use similarly batched & deferred upload of all dynamic primitives / instances (also more efficient).
	if (!InstanceData.ValidInstance || dot(InstanceData.LocalBoundsExtent, InstanceData.LocalBoundsExtent) <= 0.0f)
	{
		return true;
	}

	// TODO: impact? make permutation for shadow views? Or use a dynamic branch?
	bool bNearClip = true;

	// TODO: remove this indirection and go straight to data index
	checkStructuredBufferAccessSlow(ViewIds, ViewIdIndex);

	uint ViewDataIndex = ViewIds[ViewIdIndex];

	if (ViewDataIndex < NumCullingViews)
	{
		FNaniteView NaniteView = GetNaniteView(ViewDataIndex);

		float4x4 LocalToTranslatedWorld = InstanceData.LocalToWorld;
		LocalToTranslatedWorld[3].xyz += NaniteView.PreViewTranslation.xyz;
		float4x4 LocalToClip = mul(LocalToTranslatedWorld, NaniteView.TranslatedWorldToClip);

		FFrustumCullData Cull = BoxCullFrustum(InstanceData.LocalBoundsCenter, InstanceData.LocalBoundsExtent, LocalToClip, bNearClip, false);

#if OCCLUSION_CULL_INSTANCES
		BRANCH
		if (Cull.bIsVisible)
		{
			float4x4 LocalToPrevTranslatedWorld = InstanceData.LocalToWorld;
			LocalToPrevTranslatedWorld[3].xyz += NaniteView.PrevPreViewTranslation.xyz;
			float4x4 LocalToPrevClip = mul(LocalToPrevTranslatedWorld, NaniteView.PrevTranslatedWorldToClip);
			FFrustumCullData PrevCull = BoxCullFrustum(InstanceData.LocalBoundsCenter, InstanceData.LocalBoundsExtent, LocalToPrevClip, bNearClip, false);
			BRANCH
			if ((PrevCull.bIsVisible || PrevCull.bFrustumSideCulled) && !PrevCull.bCrossesNearPlane)
			{
				FScreenRect PrevRect = GetScreenRect( NaniteView.HZBTestViewRect, PrevCull, 4 );
				Cull.bIsVisible = IsVisibleHZB( PrevRect, true );
			}
		}
#endif

		return Cull.bIsVisible;
	}
#endif // CULL_INSTANCES
	return true;
}


/**
 * Each thread loops over a range on instances loaded from a buffer. The instance bounds are projected to all cached virtual shadow map address space
 * and any overlapped pages are marked as invalid.
 */
[numthreads(NUM_THREADS_PER_GROUP, 1, 1)]
void InstanceCullBuildInstanceIdBufferCS(uint3 GroupId : SV_GroupID, int GroupThreadIndex : SV_GroupIndex)
{
	uint DispatchGroupId = GetUnWrappedDispatchGroupId(GroupId);

	if (DispatchGroupId >= InstanceCullingLoadBalancer_GetNumBatches())
	{
		return;
	}

#if ENABLE_BATCH_MODE
	// Load Instance culling context batch info, indirection per group
	FContextBatchInfo BatchInfo = BatchInfos[BatchInds[DispatchGroupId]];
#else // !ENABLE_BATCH_MODE
	// Single Instance culling context batch in the call, set up batch from the kernel parameters
	FContextBatchInfo BatchInfo = (FContextBatchInfo)0;
	BatchInfo.NumViewIds = NumViewIds;
	BatchInfo.DynamicInstanceIdOffset = DynamicInstanceIdOffset;
	BatchInfo.DynamicInstanceIdMax = DynamicInstanceIdMax;
#endif // ENABLE_BATCH_MODE

	FInstanceWorkSetup WorkSetup = InstanceCullingLoadBalancer_Setup(GroupId, GroupThreadIndex, BatchInfo.ItemDataOffset[CurrentBatchProcessingMode]);

	if (!WorkSetup.bValid)
	{
		return;
	}

	// Check dynamic flag
	const bool bDynamicInstanceDataOffset = (WorkSetup.Item.Payload & 1U) != 0U;
	uint InstanceDataOffset = WorkSetup.Item.InstanceDataOffset;

	if (bDynamicInstanceDataOffset)
	{
		InstanceDataOffset += BatchInfo.DynamicInstanceIdOffset;
		checkSlow(InstanceDataOffset + uint(WorkSetup.LocalItemIndex) < BatchInfo.DynamicInstanceIdMax);
	}

	uint InstanceId = InstanceDataOffset + uint(WorkSetup.LocalItemIndex);

	// Work item payload stores the IndirectArgsOffset, which must be offset for the batch.
	uint IndirectArgIndex = BatchInfo.IndirectArgOffset + (WorkSetup.Item.Payload >> 1U);

	// TODO: Add unpack and store as bit flags
	// Load auxiliary per-instanced-draw command info
	FDrawCommandDesc DrawCommandDesc = DrawCommandDescs[IndirectArgIndex];

	// TODO: This must be read-modify-written when batching such that the final offset that is fed to the VS is correct.
	//       Then we don't need to add the batch offset (BatchInfo.InstanceDataWriteOffset)
	uint InstanceDataOutputOffset = InstanceIdOffsetBuffer[IndirectArgIndex];

	FInstanceSceneData InstanceData = GetInstanceSceneData(InstanceId, InstanceSceneDataSOAStride);

	if (!ShouldRenderInstance(InstanceData, DrawCommandDesc))
	{
		return;
	}

#if STEREO_CULLING_MODE
	if (IsInstanceVisible(InstanceData, BatchInfo.ViewIdsOffset + 0U) || IsInstanceVisible(InstanceData, BatchInfo.ViewIdsOffset + 1U))
	{
		uint OutputOffset = 0U;
		InterlockedAdd(DrawIndirectArgsBufferOut[IndirectArgIndex * INDIRECT_ARGS_NUM_WORDS + 1], 2U, OutputOffset);

		WriteInstance(InstanceDataOutputOffset + OutputOffset + 0U, InstanceId, InstanceData, 0U);
		WriteInstance(InstanceDataOutputOffset + OutputOffset + 1U, InstanceId, InstanceData, 1U);
	}
#else // !STEREO_CULLING_MODE

	for (uint ViewIdIndex = 0; ViewIdIndex < BatchInfo.NumViewIds; ++ViewIdIndex)
	{
		// Culling is disabled for multi-view
		if (IsInstanceVisible(InstanceData, BatchInfo.ViewIdsOffset + ViewIdIndex))
		{
			uint OutputOffset = 0;
			// TOOD: if all items in the group-batch target the same draw args the more efficient warp-collective functions can be used
			//       detected as FInstanceBatch.NumItems == 1. Can switch dynamically or bin the items that fill a group and dispatch separately with permutation.
			// TODO: if the arg only has a single item, and culling is not enabled, then we can skip the atomics. Again do dynamically or separate permutation.
			InterlockedAdd(DrawIndirectArgsBufferOut[IndirectArgIndex * INDIRECT_ARGS_NUM_WORDS + 1], 1U, OutputOffset);

			WriteInstance(InstanceDataOutputOffset + OutputOffset, InstanceId, InstanceData, ViewIdIndex);
		}
	}
#endif // STEREO_CULLING_MODE
}


uint NumIndirectArgs;
/**
 */
[numthreads(NUM_THREADS_PER_GROUP, 1, 1)]
void ClearIndirectArgInstanceCountCS(uint3 GroupId : SV_GroupID, int GroupThreadIndex : SV_GroupIndex)
{
	uint IndirectArgIndex = GetUnWrappedDispatchThreadId(GroupId, GroupThreadIndex, NUM_THREADS_PER_GROUP);

	if (IndirectArgIndex < NumIndirectArgs)
	{
		DrawIndirectArgsBufferOut[IndirectArgIndex * INDIRECT_ARGS_NUM_WORDS + 1] = 0U;
	}
}