// Copyright Epic Games, Inc. All Rights Reserved.

#include "../Common.ush"
#include "../SceneData.ush"
#include "NaniteDataDecode.ush"
#include "../VirtualShadowMaps/PageAccessCommon.ush"

// Must match ERasterTechnique in NaniteRender.h
#define RASTER_TECHNIQUE_FALLBACK	0
#define RASTER_TECHNIQUE_PLATFORM	1
#define RASTER_TECHNIQUE_NVIDIA		2
#define RASTER_TECHNIQUE_AMD_DX11	3
#define RASTER_TECHNIQUE_AMD_DX12	4
#define RASTER_TECHNIQUE_DEPTHONLY	5

#if RASTER_TECHNIQUE == RASTER_TECHNIQUE_FALLBACK
#define COHERENCY_FLAG globallycoherent
#else
#define COHERENCY_FLAG 
#endif

// When near clipping is disabled we need to move the geometry 
#ifndef NEAR_CLIP
#define NEAR_CLIP 1
#endif

RWTexture2D<uint>						OutDepthBuffer : register(u0);
COHERENCY_FLAG RWTexture2D<UlongType>	OutVisBuffer64 : register(u0);
#if DEBUG_VISUALIZE
COHERENCY_FLAG RWTexture2D<UlongType>	OutDbgBuffer64 : register(u1);
RWTexture2D<uint>						OutDbgBuffer32 : register(u2);
#endif
RWTexture2D<uint> 						LockBuffer : register(u3);

StructuredBuffer< uint2 >				InTotalPrevDrawClusters;

#if RASTER_TECHNIQUE == RASTER_TECHNIQUE_NVIDIA
#define NV_SHADER_EXTN_SLOT u7
#include "/Engine/Shared/ThirdParty/NVIDIA/nvHLSLExtns.h"

#elif RASTER_TECHNIQUE == RASTER_TECHNIQUE_AMD_DX11
#define AmdDxExtShaderIntrinsicsUAVSlot u7
#include "/Engine/Shared/ThirdParty/AMD/ags_shader_intrinsics_dx11.h"

#elif RASTER_TECHNIQUE == RASTER_TECHNIQUE_AMD_DX12
#include "/Engine/Shared/ThirdParty/AMD/ags_shader_intrinsics_dx12.h"
#endif

uint RasterStateReverseCull;

#if DEBUG_VISUALIZE
int4 VisualizeConfig;

uint2 GetVisualizeValues()
{
	uint DebugValueMax; // InterlockedMax64 using depth (value associated with surviving fragment)
	uint DebugValueAdd; // InterlockedAdd32 (value accumulated with every evaluated fragment)
	const uint VisualizeMode = VisualizeConfig.x;
	if (VisualizeMode == VISUALIZE_HW_VS_SW)
	{
	#if SOFTWARE_RASTER
		DebugValueMax = 2; // Software Raster
	#else
		DebugValueMax = 1; // Hardware Raster
	#endif
		DebugValueAdd = 0;
	}
	else if (VisualizeMode == VISUALIZE_OVERDRAW)
	{
		DebugValueMax = 0;
		DebugValueAdd = 1;
	}
	else
	{
		DebugValueMax = 0;
		DebugValueAdd = 0;
	}

	return uint2(DebugValueMax, DebugValueAdd);
}
#endif

void WritePixel( RWTexture2D<UlongType> OutBuffer, uint PixelValue, uint2 PixelPos, float DeviceZ, FNaniteView NaniteView, bool bUsePageTable )
{
#if VIRTUAL_TEXTURE_TARGET
	if( bUsePageTable )
	{
		if( !VirtualToPhysicalTexel(NaniteView.TargetLayerIndex, NaniteView.TargetMipLevel, PixelPos, PixelPos) )
		{
			// mapped to non-commited space.
			return;
		}
	}
#endif

#if !NEAR_CLIP
	DeviceZ = max(0.0f, DeviceZ);
#endif

	const uint DepthInt = asuint(DeviceZ);

#if RASTER_TECHNIQUE == RASTER_TECHNIQUE_DEPTHONLY
	InterlockedMax( OutDepthBuffer[ PixelPos ], DepthInt );
#else
	const UlongType Pixel = PackUlongType(uint2(PixelValue, DepthInt));

#if RASTER_TECHNIQUE == RASTER_TECHNIQUE_PLATFORM
#if COMPILER_SUPPORTS_UINT64_IMAGE_ATOMICS
	ImageInterlockedMaxUInt64(OutBuffer, PixelPos, Pixel);
#else
	#error UNKNOWN_ATOMIC_PLATFORM
#endif
#elif RASTER_TECHNIQUE == RASTER_TECHNIQUE_NVIDIA
	NvInterlockedMaxUint64(OutBuffer, PixelPos, Pixel); 
#elif RASTER_TECHNIQUE == RASTER_TECHNIQUE_AMD_DX11
	AmdDxExtShaderIntrinsics_AtomicMaxU64(OutBuffer, PixelPos, Pixel);
#elif RASTER_TECHNIQUE == RASTER_TECHNIQUE_AMD_DX12
	AmdExtD3DShaderIntrinsics_AtomicMaxU64(OutBuffer, PixelPos, Pixel);
#else
	// YOLO SYNC: Surprisingly this seems to work in practice (for compute, not pixel). Tested on 2080Ti and 1080.
	uint PrevDepth;
	InterlockedMax(LockBuffer[PixelPos], DepthInt, PrevDepth);
	if (DepthInt > PrevDepth)
	{
		OutBuffer[PixelPos] = Pixel;
	}
#endif
#endif
}

void RasterizeTri(
	FNaniteView NaniteView,
	int4 ViewRect,
	uint PixelValue,
#if DEBUG_VISUALIZE
	uint2 DebugValues,
#endif
	float3 Verts[3],
	bool bUsePageTable )
{
	float3 v01 = Verts[1] - Verts[0];
	float3 v02 = Verts[2] - Verts[0];

	float DetXY = v01.x * v02.y - v01.y * v02.x;
	if( DetXY >= 0.0f )
	{
		// Backface
		// If not culling, need to swap verts to correct winding for rest of code
		return;
	}

	float InvDet = rcp( DetXY );
	float2 GradZ;
	GradZ.x = ( v01.z * v02.y - v01.y * v02.z ) * InvDet;
	GradZ.y = ( v01.x * v02.z - v01.z * v02.x ) * InvDet;

	// 16.8 fixed point
	float2 Vert0 = Verts[0].xy;
	float2 Vert1 = Verts[1].xy;
	float2 Vert2 = Verts[2].xy;

	// Bounding rect
	const float2 MinSubpixel = min3( Vert0, Vert1, Vert2 );
	const float2 MaxSubpixel = max3( Vert0, Vert1, Vert2 );

#if 0
	bool2 bMissCenter =	( MinSubpixel & SUBPIXEL_MASK > (SUBPIXEL_SAMPLES / 2) ) &&
						( MaxSubpixel - ( MinSubpixel & ~SUBPIXEL_MASK ) + (SUBPIXEL_SAMPLES / 2) ) < SUBPIXEL_MASK;
	if( any( bMissCenter ) )
		return;
#endif

	// Round to nearest pixel
	int2 MinPixel = (int2)floor( ( MinSubpixel + (SUBPIXEL_SAMPLES / 2) - 1 ) * (1.0 / SUBPIXEL_SAMPLES) );
	int2 MaxPixel = (int2)floor( ( MaxSubpixel - (SUBPIXEL_SAMPLES / 2) - 1 ) * (1.0 / SUBPIXEL_SAMPLES) );	// inclusive!

	// Clip to viewport
	MinPixel = max( MinPixel, ViewRect.xy );
	MaxPixel = min( MaxPixel, ViewRect.zw - 1 );
	
	// Force 1 pixel
	//MaxPixel = max( MaxPixel, MinPixel );
	
	// Cull when no pixels covered
	if( any( MinPixel > MaxPixel ) )
		return;

	// Limit the rasterizer bounds to a sensible max.
	MaxPixel = min( MaxPixel, MinPixel + 63 );

	// 4.8 fixed point
	float2 Edge01 = -v01.xy;
	float2 Edge12 = Vert1 - Vert2;
	float2 Edge20 = v02.xy;
	
	// Rebase off MinPixel with half pixel offset
	// 4.8 fixed point
	// Max triangle size = 127x127 pixels
	const float2 BaseSubpixel = (float2)MinPixel * SUBPIXEL_SAMPLES + (SUBPIXEL_SAMPLES / 2);
	Vert0 -= BaseSubpixel;
	Vert1 -= BaseSubpixel;
	Vert2 -= BaseSubpixel;

	// Half-edge constants
	// 8.16 fixed point
	float C0 = Edge01.y * Vert0.x - Edge01.x * Vert0.y;
	float C1 = Edge12.y * Vert1.x - Edge12.x * Vert1.y;
	float C2 = Edge20.y * Vert2.x - Edge20.x * Vert2.y;

	// Correct for fill convention
	// Top left rule for CCW
#if 1
	C0 -= saturate(Edge01.y + saturate(1.0f - Edge01.x));
	C1 -= saturate(Edge12.y + saturate(1.0f - Edge12.x));
	C2 -= saturate(Edge20.y + saturate(1.0f - Edge20.x));
#else
	C0 -= ( Edge01.y < 0 || ( Edge01.y == 0 && Edge01.x > 0 ) ) ? 0 : 1;
	C1 -= ( Edge12.y < 0 || ( Edge12.y == 0 && Edge12.x > 0 ) ) ? 0 : 1;
	C2 -= ( Edge20.y < 0 || ( Edge20.y == 0 && Edge20.x > 0 ) ) ? 0 : 1;
#endif

	float Z0 = Verts[0].z - ( GradZ.x * Vert0.x + GradZ.y * Vert0.y );
	GradZ *= SUBPIXEL_SAMPLES;
	
	// Step in pixel increments
	// 8.16 fixed point
	//Edge01 *= SubpixelSamples;
	//Edge12 *= SubpixelSamples;
	//Edge20 *= SubpixelSamples;

	// Scale C0/C1/C2 down by SubpixelSamples instead of scaling Edge01/Edge12/Edge20 up. Lossless because SubpixelSamples is a power of two.
	float CY0 = C0 * (1.0f / SUBPIXEL_SAMPLES);
	float CY1 = C1 * (1.0f / SUBPIXEL_SAMPLES);
	float CY2 = C2 * (1.0f / SUBPIXEL_SAMPLES);
	float ZY = Z0;

#if COMPILER_SUPPORTS_WAVE_VOTE
	bool bScanLine = WaveAnyTrue( MaxPixel.x - MinPixel.x > 4 );
#else
	bool bScanLine = false;
#endif
	if( bScanLine )
	{
		float3 Edge012 = { Edge01.y, Edge12.y, Edge20.y };
		bool3 bOpenEdge = Edge012 < 0;
		float3 InvEdge012 = Edge012 == 0 ? 1e8 : rcp( Edge012 );

		int y = MinPixel.y;
		while( true )
		{
			//float CX0 = CY0 - Edge01.y * (x - MinPixel.x);
			// Edge01.y * (x - MinPixel.x) <= CY0;

			/*
			if( Edge01.y > 0 )
				x <= CY0 / Edge01.y + MinPixel.x;	// Closing edge
			else
				x >= CY0 / Edge01.y + MinPixel.x;	// Opening edge
			*/
			
			// No longer fixed point
			float3 CrossX = float3( CY0, CY1, CY2 ) * InvEdge012;

			float3 MinX = bOpenEdge ? CrossX : 0;
			float3 MaxX = bOpenEdge ? MaxPixel.x - MinPixel.x : CrossX;

			float x0 = ceil( max3( MinX.x, MinX.y, MinX.z ) );
			float x1 = min3( MaxX.x, MaxX.y, MaxX.z );
			float ZX = ZY + GradZ.x * x0;

			x0 += MinPixel.x;
			x1 += MinPixel.x;
			for( float x = x0; x <= x1; x++ )
			{
				WritePixel( OutVisBuffer64, PixelValue, uint2(x,y), ZX, NaniteView, bUsePageTable );
				#if DEBUG_VISUALIZE
					WritePixel( OutDbgBuffer64, DebugValues.x, uint2(x,y), ZX, NaniteView, bUsePageTable );
					InterlockedAdd(OutDbgBuffer32[ uint2(x,y) ], DebugValues.y);
				#endif

				ZX += GradZ.x;
			}

			if( y >= MaxPixel.y )
				break;

			CY0 += Edge01.x;
			CY1 += Edge12.x;
			CY2 += Edge20.x;
			ZY += GradZ.y;
			y++;
		}
	}
	else
	{
		int y = MinPixel.y;

		while (true)
		{
			int x = MinPixel.x;
			if (min3(CY0, CY1, CY2) >= 0)
			{
				WritePixel( OutVisBuffer64, PixelValue, uint2(x, y), ZY, NaniteView, bUsePageTable );
			#if DEBUG_VISUALIZE
					WritePixel( OutDbgBuffer64, DebugValues.x, uint2(x, y), ZY, NaniteView, bUsePageTable );
					InterlockedAdd(OutDbgBuffer32[uint2(x, y)], DebugValues.y);
			#endif
			}

			if (x < MaxPixel.x)
			{
				float CX0 = CY0 - Edge01.y;
				float CX1 = CY1 - Edge12.y;
				float CX2 = CY2 - Edge20.y;
				float ZX = ZY + GradZ.x;
				x++;

				HOIST_DESCRIPTORS
				while (true)
				{
					if (min3(CX0, CX1, CX2) >= 0)
					{
						WritePixel( OutVisBuffer64, PixelValue, uint2(x, y), ZX, NaniteView, bUsePageTable );
					#if DEBUG_VISUALIZE
							WritePixel( OutDbgBuffer64, DebugValues.x, uint2(x, y), ZX, NaniteView, bUsePageTable );
							InterlockedAdd(OutDbgBuffer32[uint2(x, y)], DebugValues.y);
					#endif
					}

					if (x >= MaxPixel.x)
						break;

					CX0 -= Edge01.y;
					CX1 -= Edge12.y;
					CX2 -= Edge20.y;
					ZX += GradZ.x;
					x++;
				}
			}

			if (y >= MaxPixel.y)
				break;

			CY0 += Edge01.x;
			CY1 += Edge12.x;
			CY2 += Edge20.x;
			ZY += GradZ.y;
			y++;
		}
	}
}

#if USE_CONSTRAINED_CLUSTERS
groupshared float3 GroupVerts[256];
#else
groupshared float3 GroupVerts[384];
#endif

uint GetPackedPosition(uint VertIndex, FTriCluster Cluster)
{
	const uint CompileTimeBitsPerVertex = 3 * POSITION_QUANTIZATION_BITS;
	const uint BitsPerVertex = 3 * POSITION_QUANTIZATION_BITS;
	
	FBitStreamReaderState BitStream = BitStreamReader_Create(ClusterPageData, Cluster.PageBaseAddress + Cluster.PositionOffset, VertIndex * BitsPerVertex, CompileTimeBitsPerVertex);
	return BitStreamReader_Read(BitStream, BitsPerVertex, CompileTimeBitsPerVertex);
}

float3 DecodePackedPosition(uint PackedPos, FTriCluster Cluster)
{
	const uint Bits = POSITION_QUANTIZATION_BITS;
	const uint Mask = (1u << Bits) - 1u;
	const uint3 PosData = uint3(PackedPos & Mask, (PackedPos >> Bits) & Mask, (PackedPos >> (Bits * 2)) & Mask) + Cluster.QuantizedPosStart;
	const float3 PointLocal = PosData * Cluster.MeshBoundsDelta + Cluster.MeshBoundsMin;
	return PointLocal;
}

float3 DecodePosition(uint VertIndex, FTriCluster Cluster)
{
	const uint PackedPos = GetPackedPosition(VertIndex, Cluster);
	return DecodePackedPosition(PackedPos, Cluster);
}

float3 TransformVertex(uint VertIndex, FTriCluster Cluster, float4x4 LocalToSubpixel)
{
	// Assume proper bounds checking already performed.
	float3 PointLocal = DecodePosition(VertIndex, Cluster);
	float4 PointClipSubpixel = mul(float4( PointLocal, 1), LocalToSubpixel);
	float3 Subpixel = PointClipSubpixel.xyz / PointClipSubpixel.w;
	return float3(floor(Subpixel.xy), Subpixel.z);
}

void TransformVertexCache(uint VertIndex, FTriCluster Cluster, float4x4 LocalToSubpixel, float2 SubpixelOffset)
{
	if (VertIndex < Cluster.NumVerts)
	{
		// Transform vertex and store in group shared memory.
		float3 Vert = TransformVertex(VertIndex, Cluster, LocalToSubpixel);
		Vert.xy += SubpixelOffset;
		GroupVerts[VertIndex] = Vert;
	}
}

// Default cull mode is CW. If this returns true, CCW culling is required
bool ReverseWindingOrder(FInstanceSceneData InstanceData)
{
	// Negative determinant sign for non uniform scale means
	// that an odd number of components are negative, so
	// we need to reverse the triangle winding order.
	bool bReverseInstanceCull = (InstanceData.InvNonUniformScaleAndDeterminantSign.w < 0.0f);
	bool bRasterStateReverseCull = (RasterStateReverseCull != 0);
	
	// Logical XOR
	return (bReverseInstanceCull != bRasterStateReverseCull);
}

Buffer<uint> InClusterOffsetSWHW;
groupshared float4x4 LocalToSubpixelLDS;

[numthreads(128, 1, 1)]
void MicropolyRasterize(
	uint	VisibleIndex	: SV_GroupID,
	uint	ThreadIndex		: SV_GroupIndex) 
{
#if HAS_PREV_DRAW_DATA
	VisibleIndex += InTotalPrevDrawClusters[0].x;
#endif
#if ADD_CLUSTER_OFFSET
	VisibleIndex += InClusterOffsetSWHW[0];
#endif

	// Should be all scalar.
	FVisibleCluster VisibleCluster = GetVisibleCluster( VisibleIndex, VIRTUAL_TEXTURE_TARGET );
	FInstanceSceneData InstanceData = GetInstanceData( VisibleCluster.InstanceId );
	FNaniteView NaniteView = GetNaniteView( VisibleCluster.ViewId );

	float2 SubpixelOffset = 0;

#if CLUSTER_PER_PAGE
	// Scalar
	uint2 vPage = VisibleCluster.vPage;
	uint2 pPage = PageTable[ CalcPageTableLevelOffset( NaniteView.TargetLayerIndex, NaniteView.TargetMipLevel ) + CalcPageOffsetInLevel( NaniteView.TargetMipLevel, vPage ) ];
	SubpixelOffset = ( (float2)pPage - (float2)vPage ) * VSM_PAGE_SIZE * SUBPIXEL_SAMPLES;
#endif


	float4x4 LocalToSubpixel;

	// Cannot cache the instance data with multi-view as the data is view dependent
#if SUPPORT_CACHE_INSTANCE_DYNAMIC_DATA && NANITE_MULTI_VIEW == 0
	if (RenderFlags & RENDER_FLAG_CACHE_INSTANCE_DYNAMIC_DATA)
	{
		LocalToSubpixel = GetInstanceDynamicData( VisibleCluster.InstanceId ).LocalToSubpixel;
	}
	else
#endif
	{
		// InstancedDynamicData is group invariant, so let's just calculate it once and share it through LDS.
		if(ThreadIndex == 0)
			LocalToSubpixelLDS = CalculateInstanceDynamicData(NaniteView, InstanceData).LocalToSubpixel;
		GroupMemoryBarrierWithGroupSync();
		LocalToSubpixel = LocalToSubpixelLDS;
	}
	FTriCluster Cluster = GetCluster(VisibleCluster.PageIndex, VisibleCluster.ClusterIndex);

	TransformVertexCache( ThreadIndex +   0, Cluster, LocalToSubpixel, SubpixelOffset );
	TransformVertexCache( ThreadIndex + 128, Cluster, LocalToSubpixel, SubpixelOffset );
#if USE_CONSTRAINED_CLUSTERS == 0
	TransformVertexCache( ThreadIndex + 256, Cluster, LocalToSubpixel, SubpixelOffset );
#endif
	
	GroupMemoryBarrierWithGroupSync();

	if (ThreadIndex < Cluster.NumTris)
	{
		uint TriangleID = ThreadIndex;
		uint3 TriangleIndices = ReadTriangleIndices(Cluster, TriangleID);
		if (ReverseWindingOrder(InstanceData))
		{
			TriangleIndices = uint3(TriangleIndices.x, TriangleIndices.z, TriangleIndices.y);
		}

		float3 Vertices[3];
		Vertices[0] = GroupVerts[TriangleIndices.x];
		Vertices[1] = GroupVerts[TriangleIndices.y];
		Vertices[2] = GroupVerts[TriangleIndices.z];

		uint PixelValue = ((VisibleIndex + 1) << 7) | TriangleID;
		
		int4 ViewRect = NaniteView.ViewRect;

#if CLUSTER_PER_PAGE
		ViewRect.xy = pPage * VSM_PAGE_SIZE;
		ViewRect.zw = ViewRect.xy + VSM_PAGE_SIZE;
#endif

		RasterizeTri(
			NaniteView,
			ViewRect,
			PixelValue,
		#if DEBUG_VISUALIZE
			GetVisualizeValues(),
		#endif
			Vertices,
			!CLUSTER_PER_PAGE );
	}
}

#define PIXEL_VALUE					(RASTER_TECHNIQUE != RASTER_TECHNIQUE_DEPTHONLY)
#define VERTEX_TO_TRIANGLE_MASKS	(NANITE_PRIM_SHADER && PIXEL_VALUE)

struct VSOut
{
	noperspective  float DeviceZ			: TEXCOORD0;
#if PIXEL_VALUE
	nointerpolation uint PixelValue			: TEXCOORD1;
#endif
#if NANITE_MULTI_VIEW
	nointerpolation int4 ViewRect			: TEXCOORD2;
#endif
#if DEBUG_VISUALIZE
	nointerpolation uint2 DebugValues		: TEXCOORD3;
#endif
#if VIRTUAL_TEXTURE_TARGET
	nointerpolation int ViewId				: TEXCOORD4;
#endif
#if VERTEX_TO_TRIANGLE_MASKS
	CUSTOM_INTERPOLATION uint4 ToTriangleMasks	: TEXCOORD5;
#endif
	float4 Position							: SV_Position;
};

VSOut CommonRasterizerVS(FNaniteView NaniteView, FInstanceSceneData InstanceData, FVisibleCluster VisibleCluster, FTriCluster Cluster, uint VertIndex, out float4 PointClipNoScaling)
{
	VSOut Out;

	float4x4 LocalToWorld = InstanceData.LocalToWorld;

	float3 PointLocal = DecodePosition( VertIndex, Cluster );
	float3 PointRotated = LocalToWorld[0].xyz * PointLocal.xxx + LocalToWorld[1].xyz * PointLocal.yyy + LocalToWorld[2].xyz * PointLocal.zzz;
	float3 PointTranslatedWorld = PointRotated + (LocalToWorld[3].xyz + NaniteView.PreViewTranslation.xyz);
	float4 PointClip = mul( float4( PointTranslatedWorld, 1 ), NaniteView.TranslatedWorldToClip );
	PointClipNoScaling = PointClip;
#if CLUSTER_PER_PAGE
	/*
	float2 vUV = PointClip.xy * float2(0.5, -0.5) + 0.5 * PointClip.w;
	float2 vPixels = vUV * NaniteView.ViewSizeAndInvSize.xy;
	float2 LocalPixels = vPixels - VisibleCluster.vPage * VSM_PAGE_SIZE * PointClip.w;
	float2 LocalUV = LocalPixels / ( 4 * VSM_PAGE_SIZE );
	float2 LocalClip = LocalUV * float2(2, -2) + float2(-1, 1) * PointClip.w;
	PointClip.xy = LocalClip;
	*/
	PointClip.xy = NaniteView.ClipSpaceScaleOffset.xy * PointClip.xy + NaniteView.ClipSpaceScaleOffset.zw * PointClip.w;

	// Offset 0,0 to be at vPage for a 0, VSM_PAGE_SIZE * VSM_RASTER_WINDOW_PAGES viewport.
	PointClip.xy += PointClip.w * ( float2(-2, 2) / VSM_RASTER_WINDOW_PAGES ) * VisibleCluster.vPage;

	Out.ViewRect.xy = VisibleCluster.vPage * VSM_PAGE_SIZE;
	Out.ViewRect.zw = NaniteView.ViewRect.zw;
#elif NANITE_MULTI_VIEW
	PointClip.xy = NaniteView.ClipSpaceScaleOffset.xy * PointClip.xy + NaniteView.ClipSpaceScaleOffset.zw * PointClip.w;
	Out.ViewRect = NaniteView.ViewRect;
#endif
#if VIRTUAL_TEXTURE_TARGET
	Out.ViewId = VisibleCluster.ViewId;
#endif
	Out.Position = PointClip;
	Out.DeviceZ = PointClip.z / PointClip.w;

	// Shader workaround to avoid HW depth clipping. Should be replaced with rasterizer state ideally.
#if !NEAR_CLIP
	Out.Position.z = 0.5f * Out.Position.w;
#endif

#if DEBUG_VISUALIZE
	Out.DebugValues = GetVisualizeValues();
#endif
	return Out;
}

#if NANITE_PRIM_SHADER

#pragma argument(wavemode=wave64)
#pragma argument(realtypes)

struct PrimitiveInput
{
	uint Index		: PRIM_SHADER_SEM_VERT_INDEX;
	uint WaveIndex	: PRIM_SHADER_SEM_WAVE_INDEX;
};

struct PrimitiveOutput
{
	VSOut Out;

	uint PrimExport	: PRIM_SHADER_SEM_PRIM_EXPORT;
	uint VertCount	: PRIM_SHADER_SEM_VERT_COUNT;
	uint PrimCount	: PRIM_SHADER_SEM_PRIM_COUNT;
};

uint PackTriangleExport(uint3 TriangleIndices)
{
    return TriangleIndices.x | (TriangleIndices.y << 10) | (TriangleIndices.z << 20);
}

uint3 UnpackTriangleExport(uint Packed)
{
	const uint Index0 = (Packed & 0x3FF);
	const uint Index1 = (Packed >> 10) & 0x3FF;
	const uint Index2 = (Packed >> 20);
	return uint3(Index0, Index1, Index2);
}

#if NANITE_PRIM_SHADER_CULL

#define CULL_BACK_FACE_ZERO_AREA	1
#define CULL_MULTI_VIEW_SCISSOR		1
#define DEBUG_MULTI_VIEW_SCISSOR	0

//#pragma argument(ldsmode=wgp)
//#pragma argument(scheduler=cachethrashing)
#pragma argument(scheduler=latency)
//#pragma argument(scheduler=balanced)
//#pragma argument(scheduler=minpressure)
#pragma argument(fastprecision)
#pragma argument(unrollallloops)

// Mutually exclusive
#define CACHE_VERTEX_TRANSLATED_WORLD_POS	0
#define CACHE_VERTEX_CLIP_POS				0
#define CACHE_VERTEX_PACK_POS				1

// Forced on if CACHE_VERTEX_PACK_POS is 1.
#define PACK_THREAD_REMAPPING 1
#if !PACK_THREAD_REMAPPING
groupshared uint ThreadRemapping[384];
#endif

#define SCRATCH_CLUSTER_INDEX			0
#define SCRATCH_TRIANGLE_REMAP			4
#define SCRATCH_VERTEX_REMAP			36
#define SCRATCH_TRIANGLE_REFS			100
#define SCRATCH_VERTEX_REFS				104
// End of zero initialization
#define SCRATCH_INITIALIZE_RANGE		112
#define SCRATCH_TRIANGLE_EXPORT			SCRATCH_INITIALIZE_RANGE
#define SCRATCH_VERTEX_PACKED			240
#define SCRATCH_END						496

// Have to be updated Manually. Does not support expressions.
#if CACHE_VERTEX_CLIP_POS
	#pragma argument(reservelds=5056)						
#elif !VERTEX_TO_TRIANGLE_MASKS
	#if CACHE_VERTEX_PACK_POS
		#pragma argument(reservelds=1984)
	#else
		#pragma argument(reservelds=960)
	#endif
#else
	#pragma argument(reservelds=4096)
#endif

uint ReadLDS_Byte(uint IndexInBytes, uint OffsetInBytes)
{
	return __ds_read_u8(IndexInBytes, OffsetInBytes);
}

uint ReadLDS_UInt(uint IndexInDwords, uint OffsetInDwords)
{
	return __ds_read_u32(IndexInDwords * 4, OffsetInDwords * 4);
}

uint3 ReadLDS_UInt3( uint IndexInDwords, uint OffsetInDwords )
{
	return __ds_read_b96( IndexInDwords * 4, OffsetInDwords * 4 );
}

uint4 ReadLDS_UInt4( uint IndexInDwords, uint OffsetInDwords )
{
	return __ds_read_b128( IndexInDwords * 4, OffsetInDwords * 4 );
}

void WriteLDS_Byte(uint IndexInBytes, uint Value, uint OffsetInBytes)
{
	__ds_write_b8(IndexInBytes, Value, OffsetInBytes);
}

void WriteLDS_UInt(uint IndexInDwords, uint Value, uint OffsetInDwords)
{
	__ds_write_u32(IndexInDwords * 4, Value, OffsetInDwords * 4);
}

void WriteLDS_UInt4(uint IndexInDwords, uint4 Value, uint OffsetInDwords)
{
	__ds_write_b128(IndexInDwords * 4, Value, OffsetInDwords * 4);
}

void MapFromNewToOldTriangleID(uint NewTriangleID, uint OldTriangleID)
{
#if CACHE_VERTEX_PACK_POS || PACK_THREAD_REMAPPING
	__ds_write_b8(NewTriangleID, OldTriangleID, SCRATCH_TRIANGLE_REMAP * 4);
#else
	ThreadRemapping[NewTriangleID] = OldTriangleID;
	//WriteLDS_UInt(SCRATCH_VERTEX_PACKED, OldTriangleID, NewTriangleID);
#endif
}

uint RemapNewToOldTriangleID(uint NewTriangleID)
{
#if CACHE_VERTEX_PACK_POS || PACK_THREAD_REMAPPING
	return ReadLDS_Byte(NewTriangleID, SCRATCH_TRIANGLE_REMAP*4);
#else
	return ThreadRemapping[NewTriangleID];
	//return ReadLDS_UInt(SCRATCH_VERTEX_PACKED, NewTriangleID);
#endif
}

void MapFromNewToOldVertexID(uint NewVertexID, uint OldVertexID)
{
#if CACHE_VERTEX_PACK_POS || PACK_THREAD_REMAPPING
	__ds_write_b8(NewVertexID, OldVertexID, SCRATCH_VERTEX_REMAP * 4);
#else
	ThreadRemapping[128 + NewVertexID] = OldVertexID;
	//WriteLDS_UInt(SCRATCH_VERTEX_PACKED + 128, OldVertexID, NewVertexID);
#endif
}

uint RemapNewToOldVertexID(uint NewVertexID)
{
#if CACHE_VERTEX_PACK_POS || PACK_THREAD_REMAPPING
	return ReadLDS_Byte(NewVertexID, SCRATCH_VERTEX_REMAP*4);
#else
	return ThreadRemapping[128 + NewVertexID];
	//return ReadLDS_UInt(SCRATCH_VERTEX_PACKED + 128, NewVertexID);
#endif
}

void MarkVertexReferenced(uint Position)
{
	const uint UIntIndex = Position >> 5u;
	const uint BitIndex  = Position & 31u;
	__ds_or_b32(UIntIndex*4, 1u << BitIndex, SCRATCH_VERTEX_REFS*4);
}

bool IsVertexReferenced(uint Position, uint References[8])
{
	const uint UIntIndex = Position >> 5u;
	const uint BitIndex  = Position & 31u;
	const uint Value = References[UIntIndex];
	return (Value >> BitIndex) & 1;
}

uint GetCompactedVertex(uint Position, uint Counts[8], uint References[8])
{
	const uint UIntIndex = Position >> 5u;
	const uint BitIndex  = Position & 31u;

	uint PrefixSum = 0;
	PrefixSum += CondMask(UIntIndex > 0, Counts[0], 0);
	PrefixSum += CondMask(UIntIndex > 1, Counts[1], 0);
	PrefixSum += CondMask(UIntIndex > 2, Counts[2], 0);
	PrefixSum += CondMask(UIntIndex > 3, Counts[3], 0);
	PrefixSum += CondMask(UIntIndex > 4, Counts[4], 0);
	PrefixSum += CondMask(UIntIndex > 5, Counts[5], 0);
	PrefixSum += CondMask(UIntIndex > 6, Counts[6], 0);

	const uint Value = References[UIntIndex];
	const uint MaskedValue = Value & ((1u << BitIndex) - 1u);
	PrefixSum += popcnt(MaskedValue);

	return PrefixSum;
}

void MarkTriangleReferenced(uint Position)
{
	const uint UIntIndex = Position >> 5u;
	const uint BitIndex  = Position & 31u;
	__ds_or_b32(UIntIndex*4, 1u << BitIndex, SCRATCH_TRIANGLE_REFS * 4);
}

uint GetCompactedTriangle(uint Position, uint Counts[4], uint References[4])
{
	const uint UIntIndex = Position >> 5u;
	const uint BitIndex  = Position & 31u;

	uint PrefixSum = 0;
	PrefixSum += CondMask(UIntIndex > 0, Counts[0], 0);
	PrefixSum += CondMask(UIntIndex > 1, Counts[1], 0);
	PrefixSum += CondMask(UIntIndex > 2, Counts[2], 0);
	const uint MaskedValue = References[UIntIndex] & ((1u << BitIndex) - 1u);
	PrefixSum += popcnt(MaskedValue);

	return PrefixSum;
}

float3 GetClusterPointTranslatedWorld(uint PackedPos, FNaniteView NaniteView, FInstanceSceneData InstanceData, FTriCluster Cluster)
{
	const float3 PointLocal = DecodePackedPosition(PackedPos, Cluster);

	const float3 PointRotated = 
		InstanceData.LocalToWorld[0].xyz * PointLocal.xxx +
		InstanceData.LocalToWorld[1].xyz * PointLocal.yyy +
		InstanceData.LocalToWorld[2].xyz * PointLocal.zzz;

	return PointRotated + (InstanceData.LocalToWorld[3].xyz + NaniteView.PreViewTranslation.xyz);
}


float4 GetClusterPointClip(uint PackedPos, FNaniteView NaniteView, FInstanceSceneData InstanceData, FTriCluster Cluster)
{
	const float3 PointTranslatedWorld = GetClusterPointTranslatedWorld(PackedPos, NaniteView, InstanceData, Cluster);
	return mul(float4(PointTranslatedWorld, 1), NaniteView.TranslatedWorldToClip);
}

PRIM_SHADER_OUTPUT_TRIANGLES
PRIM_SHADER_PRIM_COUNT(1)
PRIM_SHADER_VERT_COUNT(1)
PRIM_SHADER_VERT_LIMIT(256)
PRIM_SHADER_AMP_FACTOR(128)
PRIM_SHADER_AMP_ENABLE
PrimitiveOutput HWRasterizeVS(PrimitiveInput Input)
{
	const uint LaneIndex = WaveGetLaneIndex();
	const uint LaneCount = WaveGetLaneCount();

	const uint GroupThreadID = LaneIndex + Input.WaveIndex * LaneCount;

	if (GroupThreadID < (SCRATCH_INITIALIZE_RANGE >> 2))
	{
		// Input index is only initialized for lane 0, so we need to manually
		// communicate it to all other threads in subgroup (not just wavefront).
		const uint4 Value = uint4(CondMask(GroupThreadID == 0u, Input.Index, 0u), 0u, 0u, 0u);
		WriteLDS_UInt4( GroupThreadID * 4, Value, 0);
	}

	GroupMemoryBarrierWithGroupSync();

	uint VisibleIndex = 0;
	if (WaveOnce())
	{
		// Avoid per thread reads from LDS.
		VisibleIndex = ReadLDS_UInt(0, SCRATCH_CLUSTER_INDEX);

	#if HAS_PREV_DRAW_DATA
		VisibleIndex += InTotalPrevDrawClusters[0].y;
	#endif

	#if ADD_CLUSTER_OFFSET
		VisibleIndex += InClusterOffsetSWHW[GetHWClusterCounterIndex(RenderFlags)];
	#endif

		VisibleIndex = (MaxClusters - 1) - VisibleIndex;
	}

	// Broadcast visible index to all lanes.
	VisibleIndex = ToScalarMemory(VisibleIndex);

	// Should be all scalar.
	FVisibleCluster VisibleCluster	= GetVisibleCluster(VisibleIndex, VIRTUAL_TEXTURE_TARGET);
	FNaniteView NaniteView			= GetNaniteView(VisibleCluster.ViewId);

	// Should be scalar.
	FInstanceSceneData InstanceData	= GetInstanceData(VisibleCluster.InstanceId);

	FInstanceDynamicData InstanceDynamicData;
#if SUPPORT_CACHE_INSTANCE_DYNAMIC_DATA
	BRANCH
	if (RenderFlags & RENDER_FLAG_CACHE_INSTANCE_DYNAMIC_DATA)
	{
		InstanceDynamicData = GetInstanceDynamicData(VisibleCluster.InstanceId);
	}
	else
#else
	{
		InstanceDynamicData = CalculateInstanceDynamicData(NaniteView, InstanceData);
	}
#endif

	FTriCluster Cluster = GetCluster(VisibleCluster.PageIndex, VisibleCluster.ClusterIndex);

#if CACHE_VERTEX_TRANSLATED_WORLD_POS || CACHE_VERTEX_CLIP_POS || CACHE_VERTEX_PACK_POS
	if (GroupThreadID < Cluster.NumVerts)
	{
		const uint PackedPos = GetPackedPosition(GroupThreadID, Cluster);
#if CACHE_VERTEX_TRANSLATED_WORLD_POS
		const float3 PointTranslatedWorld = GetClusterPointTranslatedWorld(PackedPos, NaniteView, InstanceData, Cluster);
		WriteLDS_UInt(GroupThreadID*3, asuint(PointTranslatedWorld.x), SCRATCH_VERTEX_PACKED+0);
		WriteLDS_UInt(GroupThreadID*3, asuint(PointTranslatedWorld.y), SCRATCH_VERTEX_PACKED+1);
		WriteLDS_UInt(GroupThreadID*3, asuint(PointTranslatedWorld.z), SCRATCH_VERTEX_PACKED+2);
#elif CACHE_VERTEX_CLIP_POS
		float4 PointClip = GetClusterPointClip(PackedPos, NaniteView, InstanceData, Cluster);
	#if CLUSTER_PER_PAGE
		PointClip.xy = NaniteView.ClipSpaceScaleOffset.xy * PointClip.xy + NaniteView.ClipSpaceScaleOffset.zw * PointClip.w;

		// Offset 0,0 to be at vPage for a 0, VSM_PAGE_SIZE * VSM_RASTER_WINDOW_PAGES viewport.
		PointClip.xy += PointClip.w * (float2(-2, 2) / VSM_RASTER_WINDOW_PAGES) * VisibleCluster.vPage;
	#endif
		WriteLDS_UInt4(GroupThreadID*4, asuint(PointClip), SCRATCH_VERTEX_PACKED);
#elif CACHE_VERTEX_PACK_POS
		WriteLDS_UInt(GroupThreadID, PackedPos, SCRATCH_VERTEX_PACKED);
#endif
	}

	GroupMemoryBarrierWithGroupSync();
#endif

	bool bCullTriangle = false;

	if (GroupThreadID < Cluster.NumTris)
	{
		const uint OldTriangleID = GroupThreadID;
		uint3 TriangleIndices = ReadTriangleIndices(Cluster, OldTriangleID);
		if (ReverseWindingOrder(InstanceData))
		{
			TriangleIndices = uint3(TriangleIndices.x, TriangleIndices.z, TriangleIndices.y);
		}


	#if CACHE_VERTEX_TRANSLATED_WORLD_POS
		uint Point0Offset = TriangleIndices.x * 3;
		uint Point1Offset = TriangleIndices.y * 3;
		uint Point2Offset = TriangleIndices.z * 3;
		//float4 Point0TranslatedWorld = asfloat(ReadLDS_UInt3(Point0Offset, SCRATCH_VERTEX_PACKED));	//TODO: Seems to result in broken codegen. Investigate
		//float4 Point1TranslatedWorld = asfloat(ReadLDS_UInt3(Point1Offset, SCRATCH_VERTEX_PACKED));
		//float4 Point2TranslatedWorld = asfloat(ReadLDS_UInt3(Point2Offset, SCRATCH_VERTEX_PACKED));
		float3 Point0TranslatedWorld = asfloat(uint3(ReadLDS_UInt(Point0Offset, SCRATCH_VERTEX_PACKED+0), ReadLDS_UInt(Point0Offset, SCRATCH_VERTEX_PACKED+1), ReadLDS_UInt(Point0Offset, SCRATCH_VERTEX_PACKED+2)));
		float3 Point1TranslatedWorld = asfloat(uint3(ReadLDS_UInt(Point1Offset, SCRATCH_VERTEX_PACKED+0), ReadLDS_UInt(Point1Offset, SCRATCH_VERTEX_PACKED+1), ReadLDS_UInt(Point1Offset, SCRATCH_VERTEX_PACKED+2)));
		float3 Point2TranslatedWorld = asfloat(uint3(ReadLDS_UInt(Point2Offset, SCRATCH_VERTEX_PACKED+0), ReadLDS_UInt(Point2Offset, SCRATCH_VERTEX_PACKED+1), ReadLDS_UInt(Point2Offset, SCRATCH_VERTEX_PACKED+2)));
		float4 Point0Clip = mul(float4(Point0TranslatedWorld, 1), NaniteView.TranslatedWorldToClip);
		float4 Point1Clip = mul(float4(Point1TranslatedWorld, 1), NaniteView.TranslatedWorldToClip);
		float4 Point2Clip = mul(float4(Point2TranslatedWorld, 1), NaniteView.TranslatedWorldToClip);
	#elif CACHE_VERTEX_CLIP_POS
		uint Point0Offset = TriangleIndices.x * 4;
		uint Point1Offset = TriangleIndices.y * 4;
		uint Point2Offset = TriangleIndices.z * 4;
		//float4 Point0Clip = asfloat(ReadLDS_UInt4(Point0Offset, SCRATCH_VERTEX_PACKED));		//TODO: Seems to result in broken codegen. Investigate
		//float4 Point1Clip = asfloat(ReadLDS_UInt4(Point1Offset, SCRATCH_VERTEX_PACKED));
		//float4 Point2Clip = asfloat(ReadLDS_UInt4(Point2Offset, SCRATCH_VERTEX_PACKED));
		float4 Point0Clip = asfloat(uint4(ReadLDS_UInt(Point0Offset, SCRATCH_VERTEX_PACKED+0), ReadLDS_UInt(Point0Offset, SCRATCH_VERTEX_PACKED+1), ReadLDS_UInt(Point0Offset, SCRATCH_VERTEX_PACKED+2), ReadLDS_UInt(Point0Offset, SCRATCH_VERTEX_PACKED+3)));
		float4 Point1Clip = asfloat(uint4(ReadLDS_UInt(Point1Offset, SCRATCH_VERTEX_PACKED+0), ReadLDS_UInt(Point1Offset, SCRATCH_VERTEX_PACKED+1), ReadLDS_UInt(Point1Offset, SCRATCH_VERTEX_PACKED+2), ReadLDS_UInt(Point1Offset, SCRATCH_VERTEX_PACKED+3)));
		float4 Point2Clip = asfloat(uint4(ReadLDS_UInt(Point2Offset, SCRATCH_VERTEX_PACKED+0), ReadLDS_UInt(Point2Offset, SCRATCH_VERTEX_PACKED+1), ReadLDS_UInt(Point2Offset, SCRATCH_VERTEX_PACKED+2), ReadLDS_UInt(Point2Offset, SCRATCH_VERTEX_PACKED+3)));
	#else
		#if CACHE_VERTEX_PACK_POS
			const uint Point0Packed = ReadLDS_UInt(TriangleIndices.x, SCRATCH_VERTEX_PACKED);
			const uint Point1Packed = ReadLDS_UInt(TriangleIndices.y, SCRATCH_VERTEX_PACKED);
			const uint Point2Packed = ReadLDS_UInt(TriangleIndices.z, SCRATCH_VERTEX_PACKED);
		#else
			const uint Point0Packed	= GetPackedPosition(TriangleIndices.x, Cluster);
			const uint Point1Packed	= GetPackedPosition(TriangleIndices.y, Cluster);
			const uint Point2Packed	= GetPackedPosition(TriangleIndices.z, Cluster);
		#endif
		float4 Point0Clip = GetClusterPointClip(Point0Packed, NaniteView, InstanceData, Cluster);
		float4 Point1Clip = GetClusterPointClip(Point1Packed, NaniteView, InstanceData, Cluster);
		float4 Point2Clip = GetClusterPointClip(Point2Packed, NaniteView, InstanceData, Cluster);
	#endif

	#if CULL_BACK_FACE_ZERO_AREA
		// Back face cull and zero area test.
		// "Triangle Scan Conversion using 2D Homogeneous Coordinates"
		// https://www.cs.cmu.edu/afs/cs/academic/class/15869-f11/www/readings/olano97_homogeneous.pdf
		float DetM = determinant(float3x3(Point0Clip.xyw, Point1Clip.xyw, Point2Clip.xyw));
		bCullTriangle |= DetM <= 0.0; // Avoids projection and clipping issues.
	#endif

	#if CULL_MULTI_VIEW_SCISSOR && (NANITE_MULTI_VIEW || DEBUG_MULTI_VIEW_SCISSOR)
		// In multi-view mode every view has its own scissor, so we have to scissor manually.
	#if DEBUG_MULTI_VIEW_SCISSOR
		Point0Clip.xy *= float2(2, 2);
		Point1Clip.xy *= float2(2, 2);
		Point2Clip.xy *= float2(2, 2);
	#endif

	#if CLUSTER_PER_PAGE && !CACHE_VERTEX_CLIP_POS
		Point0Clip.xy = NaniteView.ClipSpaceScaleOffset.xy * Point0Clip.xy + NaniteView.ClipSpaceScaleOffset.zw * Point0Clip.w;
		Point1Clip.xy = NaniteView.ClipSpaceScaleOffset.xy * Point1Clip.xy + NaniteView.ClipSpaceScaleOffset.zw * Point1Clip.w;
		Point2Clip.xy = NaniteView.ClipSpaceScaleOffset.xy * Point2Clip.xy + NaniteView.ClipSpaceScaleOffset.zw * Point2Clip.w;
		
		// Offset 0,0 to be at vPage for a 0, VSM_PAGE_SIZE * VSM_RASTER_WINDOW_PAGES viewport.
		Point0Clip.xy += Point0Clip.w * (float2(-2, 2) / VSM_RASTER_WINDOW_PAGES) * VisibleCluster.vPage;
		Point1Clip.xy += Point1Clip.w * (float2(-2, 2) / VSM_RASTER_WINDOW_PAGES) * VisibleCluster.vPage;
		Point2Clip.xy += Point2Clip.w * (float2(-2, 2) / VSM_RASTER_WINDOW_PAGES) * VisibleCluster.vPage;
	#endif

		bCullTriangle |= 
			(Point0Clip.x < -Point0Clip.w && Point1Clip.x < -Point1Clip.w && Point2Clip.x < -Point2Clip.w) ||
			(Point0Clip.x >  Point0Clip.w && Point1Clip.x >  Point1Clip.w && Point2Clip.x >  Point2Clip.w) ||
			(Point0Clip.y < -Point0Clip.w && Point1Clip.y < -Point1Clip.w && Point2Clip.y < -Point2Clip.w) ||
			(Point0Clip.y >  Point0Clip.w && Point1Clip.y >  Point1Clip.w && Point2Clip.y >  Point2Clip.w);
	#endif

		if (!bCullTriangle)
		{
			// Cache packed triangle export in LDS.
			WriteLDS_UInt(OldTriangleID, PackTriangleExport(TriangleIndices), SCRATCH_TRIANGLE_EXPORT);

			MarkTriangleReferenced(OldTriangleID);
			MarkVertexReferenced(TriangleIndices.x);
			MarkVertexReferenced(TriangleIndices.y);
			MarkVertexReferenced(TriangleIndices.z);
		}
	}

	GroupMemoryBarrierWithGroupSync();

	uint4 TriangleReferenceBlock;
	uint4 VertexReferenceBlocks[2];
	
	if (WaveOnce())
	{
		TriangleReferenceBlock   = ReadLDS_UInt4(0, SCRATCH_TRIANGLE_REFS);
		VertexReferenceBlocks[0] = ReadLDS_UInt4(0, SCRATCH_VERTEX_REFS);
		VertexReferenceBlocks[1] = ReadLDS_UInt4(4, SCRATCH_VERTEX_REFS);
	}

	TriangleReferenceBlock = ReadFirstLane(TriangleReferenceBlock);
	VertexReferenceBlocks[0] = ReadFirstLane(VertexReferenceBlocks[0]);
	VertexReferenceBlocks[1] = ReadFirstLane(VertexReferenceBlocks[1]);

	uint TriangleReferences[4];
	TriangleReferences[0] = TriangleReferenceBlock.x;
	TriangleReferences[1] = TriangleReferenceBlock.y;
	TriangleReferences[2] = TriangleReferenceBlock.z;
	TriangleReferences[3] = TriangleReferenceBlock.w;

	uint VertexReferences[8];
	VertexReferences[0] = VertexReferenceBlocks[0].x;
	VertexReferences[1] = VertexReferenceBlocks[0].y;
	VertexReferences[2] = VertexReferenceBlocks[0].z;
	VertexReferences[3] = VertexReferenceBlocks[0].w;
	VertexReferences[4] = VertexReferenceBlocks[1].x;
	VertexReferences[5] = VertexReferenceBlocks[1].y;
	VertexReferences[6] = VertexReferenceBlocks[1].z;
	VertexReferences[7] = VertexReferenceBlocks[1].w;

#if 0
	const uint VertexCount =
		popcnt(bit_cast<ulong>(VertexReferences[0].xy)) +
		popcnt(bit_cast<ulong>(VertexReferences[0].zw)) +
		popcnt(bit_cast<ulong>(VertexReferences[1].xy)) +
		popcnt(bit_cast<ulong>(VertexReferences[1].zw));
#else
	uint VertexCount = 0;
	uint VertexCounts[8];
	[unroll]
	for (uint CountIndex = 0; CountIndex < 8; ++CountIndex)
	{
		VertexCounts[CountIndex] = popcnt(VertexReferences[CountIndex]);
		VertexCount += VertexCounts[CountIndex];
	}

	uint TriangleCount = 0;
	uint TriangleCounts[4];
	[unroll]
	for (uint CountIndex = 0; CountIndex < 4; ++CountIndex)
	{
		TriangleCounts[CountIndex] = popcnt(TriangleReferences[CountIndex]);
		TriangleCount += TriangleCounts[CountIndex];
	}
#endif

	if (GroupThreadID < Cluster.NumTris && !bCullTriangle)
	{
		const uint OldTriangleID = GroupThreadID;
		const uint NewTriangleID = GetCompactedTriangle(OldTriangleID, TriangleCounts, TriangleReferences);
		MapFromNewToOldTriangleID(NewTriangleID, OldTriangleID);
	}

	if (GroupThreadID < Cluster.NumVerts && IsVertexReferenced(GroupThreadID, VertexReferences))
	{
		const uint OldVertexID = GroupThreadID;
		const uint NewVertexID = GetCompactedVertex(OldVertexID, VertexCounts, VertexReferences);
		MapFromNewToOldVertexID(NewVertexID, OldVertexID);
	}

	GroupMemoryBarrierWithGroupSync();

	const uint NewTriangleID = GroupThreadID;

	uint  OldTriangleID   = 0;
	uint  TriangleExport  = 0;
	uint3 TriangleIndices = uint3(0, 0, 0);

	uint3 CompactedIndices = uint3(0, 0, 0);
	if (NewTriangleID < TriangleCount)
	{
		// Pull out new to old triangle thread indirection, and packed triangle export.
		OldTriangleID   = RemapNewToOldTriangleID(NewTriangleID);
		TriangleExport	= ReadLDS_UInt(OldTriangleID, SCRATCH_TRIANGLE_EXPORT);
		TriangleIndices	= UnpackTriangleExport(TriangleExport);

		// Repack new triangle export.
		CompactedIndices.x = GetCompactedVertex(TriangleIndices.x, VertexCounts, VertexReferences);
		CompactedIndices.y = GetCompactedVertex(TriangleIndices.y, VertexCounts, VertexReferences);
		CompactedIndices.z = GetCompactedVertex(TriangleIndices.z, VertexCounts, VertexReferences);
		TriangleExport = PackTriangleExport(CompactedIndices);
	}

	const uint NewVertexID = GroupThreadID;

#if CACHE_VERTEX_PACK_POS
	uint PointPacked = 0;
#endif
	uint OldVertexID = 0;
	if (NewVertexID < Cluster.NumVerts)
	{
		// Pull out new to old vertex thread indirection.
		OldVertexID = RemapNewToOldVertexID(NewVertexID);
	#if CACHE_VERTEX_PACK_POS
		PointPacked = ReadLDS_UInt(OldVertexID, SCRATCH_VERTEX_PACKED);
	#endif
	}

#if CACHE_VERTEX_TRANSLATED_WORLD_POS
	float3 PointWorld;
	if (NewVertexID < VertexCount)
	{
		uint PointOffset = SCRATCH_VERTEX_PACKED + OldVertexID * 3;
		PointWorld = asfloat(uint3(ReadLDS_UInt(PointOffset, 0), ReadLDS_UInt(PointOffset, 1), ReadLDS_UInt(PointOffset, 2)));
	}
#elif CACHE_VERTEX_CLIP_POS
	float4 PointClip;
	if (NewVertexID < VertexCount)
	{
		const uint PointOffset = SCRATCH_VERTEX_PACKED + OldVertexID * 4;
		PointClip = asfloat(uint4(ReadLDS_UInt(PointOffset, 0), ReadLDS_UInt(PointOffset, 1), ReadLDS_UInt(PointOffset, 2), ReadLDS_UInt(PointOffset, 3)));
		//PointClip = asfloat(ReadLDS_UInt4(PointOffset, SCRATCH_VERTEX_PACKED)); // broken codegen?
	}
#endif

	// END OF SCRATCH MAPPINGS

#if VERTEX_TO_TRIANGLE_MASKS
	GroupMemoryBarrierWithGroupSync();

	if (GroupThreadID < Cluster.NumVerts)
	{
		// Every vertex has 4 dwords for vertex to triangle masks.
		WriteLDS_UInt4(GroupThreadID * 4, 0, 0);
	}

	GroupMemoryBarrierWithGroupSync();

	if (GroupThreadID < TriangleCount)
	{
		const uint DwordIndex   = (OldTriangleID >> 5) & 3;
		const uint TriangleMask = 1 << (OldTriangleID & 31);
		__ds_or_b32((CompactedIndices.x * 4 + DwordIndex) * 4, TriangleMask, 0);
		__ds_or_b32((CompactedIndices.y * 4 + DwordIndex) * 4, TriangleMask, 0);
		__ds_or_b32((CompactedIndices.z * 4 + DwordIndex) * 4, TriangleMask, 0);
	}

	GroupMemoryBarrierWithGroupSync();
#endif

	VSOut VertexExport = (VSOut)0;
	if (GroupThreadID < VertexCount)
	{
	#if DEBUG_VISUALIZE
		VertexExport.DebugValues = GetVisualizeValues();
	#endif

	#if VIRTUAL_TEXTURE_TARGET
		VertexExport.ViewId		 = VisibleCluster.ViewId;
	#endif

	#if CACHE_VERTEX_TRANSLATED_WORLD_POS
		//const float4 PointClip = mul(float4(PointWorld, 1), NaniteView.TranslatedWorldToClip);
		float4 PointClip = GetClusterPointClip(GetPackedPosition(OldVertexID, Cluster), NaniteView, InstanceData, Cluster);	//TODO: Wow, recalculating here is actually faster than fetching from LDS! Investigate!
	#elif CACHE_VERTEX_CLIP_POS	
		// Already calculated above
	#else
		#if !CACHE_VERTEX_PACK_POS
			const uint PointPacked = GetPackedPosition(OldVertexID, Cluster);
		#endif
		float4 PointClip = GetClusterPointClip(PointPacked, NaniteView, InstanceData, Cluster);
	#endif

#if CLUSTER_PER_PAGE
	#if !CACHE_VERTEX_CLIP_POS
		PointClip.xy = NaniteView.ClipSpaceScaleOffset.xy * PointClip.xy + NaniteView.ClipSpaceScaleOffset.zw * PointClip.w;

		// Offset 0,0 to be at vPage for a 0, VSM_PAGE_SIZE * VSM_RASTER_WINDOW_PAGES viewport.
		PointClip.xy += PointClip.w * (float2(-2, 2) / VSM_RASTER_WINDOW_PAGES) * VisibleCluster.vPage;
	#endif
		VertexExport.ViewRect.xy = VisibleCluster.vPage * VSM_PAGE_SIZE;
		VertexExport.ViewRect.zw = NaniteView.ViewRect.zw;
#elif NANITE_MULTI_VIEW && !CACHE_VERTEX_CLIP_POS
		VertexExport.Position.xy = NaniteView.ClipSpaceScaleOffset.xy * VertexExport.Position.xy + NaniteView.ClipSpaceScaleOffset.zw * VertexExport.Position.w;
		VertexExport.ViewRect	 = NaniteView.ViewRect;
#endif

		VertexExport.Position = PointClip;
		VertexExport.DeviceZ = VertexExport.Position.z / VertexExport.Position.w;

	#if !NEAR_CLIP
		// Shader workaround to avoid HW depth clipping.
		// Should be replaced with rasterizer state ideally.
		VertexExport.Position.z = 0.5f * VertexExport.Position.w;
	#endif
	}

#if VERTEX_TO_TRIANGLE_MASKS
	if (GroupThreadID < VertexCount)
	{
		VertexExport.PixelValue			= ((VisibleIndex + 1) << 7);
		VertexExport.ToTriangleMasks	= ReadLDS_UInt4(NewVertexID * 4, 0);
	}
#endif

	PrimitiveOutput PrimOutput = (PrimitiveOutput)0;
	PrimOutput.Out			= VertexExport;
	PrimOutput.VertCount	= VertexCount;
	PrimOutput.PrimExport	= TriangleExport;
	PrimOutput.PrimCount	= TriangleCount;
	return PrimOutput;
}

#else // NANITE_PRIM_SHADER_CULL

#if VERTEX_TO_TRIANGLE_MASKS
groupshared uint GroupVertexToTriangleMasks[256][4];
#endif
groupshared uint GroupTriangleCount;
groupshared uint GroupVertexCount;
groupshared uint GroupClusterIndex;

PRIM_SHADER_OUTPUT_TRIANGLES
PRIM_SHADER_PRIM_COUNT(1)
PRIM_SHADER_VERT_COUNT(1)
PRIM_SHADER_VERT_LIMIT(256)
PRIM_SHADER_AMP_FACTOR(128)
PRIM_SHADER_AMP_ENABLE
PrimitiveOutput HWRasterizeVS(PrimitiveInput Input)
{
	const uint LaneIndex = WaveGetLaneIndex();
	const uint LaneCount = WaveGetLaneCount();

	const uint GroupThreadID = LaneIndex + Input.WaveIndex * LaneCount;

	if (GroupThreadID == 0)
	{
		// Input index is only initialized for lane 0, so we need to manually communicate it to all other threads in subgroup (not just wavefront).
		GroupClusterIndex = Input.Index;
	}
	
	GroupMemoryBarrierWithGroupSync();

	uint VisibleIndex = GroupClusterIndex;
#if HAS_PREV_DRAW_DATA
	VisibleIndex += InTotalPrevDrawClusters[0].y;
#endif
#if ADD_CLUSTER_OFFSET
	VisibleIndex += InClusterOffsetSWHW[GetHWClusterCounterIndex(RenderFlags)];
#endif
	VisibleIndex = (MaxClusters - 1) - VisibleIndex;

	// Should be all scalar.
	FVisibleCluster VisibleCluster = GetVisibleCluster( VisibleIndex, VIRTUAL_TEXTURE_TARGET );
	FInstanceSceneData InstanceData = GetInstanceData( VisibleCluster.InstanceId );
	FNaniteView NaniteView = GetNaniteView( VisibleCluster.ViewId );

	FInstanceDynamicData InstanceDynamicData;
#if SUPPORT_CACHE_INSTANCE_DYNAMIC_DATA
	if (RenderFlags & RENDER_FLAG_CACHE_INSTANCE_DYNAMIC_DATA)
	{
		InstanceDynamicData = GetInstanceDynamicData(VisibleCluster.InstanceId);
	}
	else
#else
	{
		InstanceDynamicData = CalculateInstanceDynamicData(NaniteView, InstanceData);
	}
#endif

	FTriCluster Cluster = GetCluster(VisibleCluster.PageIndex, VisibleCluster.ClusterIndex);

#if VERTEX_TO_TRIANGLE_MASKS
	if (GroupThreadID < Cluster.NumVerts)
	{
		GroupVertexToTriangleMasks[GroupThreadID][0] = 0;
		GroupVertexToTriangleMasks[GroupThreadID][1] = 0;
		GroupVertexToTriangleMasks[GroupThreadID][2] = 0;
		GroupVertexToTriangleMasks[GroupThreadID][3] = 0;
	}
#endif

	GroupMemoryBarrierWithGroupSync();

	PrimitiveOutput PrimOutput;
	PrimOutput.VertCount = Cluster.NumVerts;
	PrimOutput.PrimCount = Cluster.NumTris;

	bool bCullTriangle = false;

	if (GroupThreadID < Cluster.NumTris)
	{
		uint TriangleID = GroupThreadID;
		uint3 TriangleIndices = ReadTriangleIndices(Cluster, TriangleID);
		if (ReverseWindingOrder(InstanceData))
		{
			TriangleIndices = uint3(TriangleIndices.x, TriangleIndices.z, TriangleIndices.y);
		}

#if VERTEX_TO_TRIANGLE_MASKS
		const uint DwordIndex   = (GroupThreadID >> 5) & 3;
		const uint TriangleMask = 1 << (GroupThreadID & 31);
		InterlockedOr(GroupVertexToTriangleMasks[TriangleIndices.x][DwordIndex], TriangleMask);
		InterlockedOr(GroupVertexToTriangleMasks[TriangleIndices.y][DwordIndex], TriangleMask);
		InterlockedOr(GroupVertexToTriangleMasks[TriangleIndices.z][DwordIndex], TriangleMask);
#endif
		PrimOutput.PrimExport = PackTriangleExport(TriangleIndices);
	}

	GroupMemoryBarrierWithGroupSync();

	if (GroupThreadID < Cluster.NumVerts)
	{
		float4 PointClipNoScaling;
		PrimOutput.Out = CommonRasterizerVS(NaniteView, InstanceData, VisibleCluster, Cluster, GroupThreadID, PointClipNoScaling);
#if VERTEX_TO_TRIANGLE_MASKS
		PrimOutput.Out.PixelValue = ((VisibleIndex + 1) << 7);
		PrimOutput.Out.ToTriangleMasks = uint4(GroupVertexToTriangleMasks[GroupThreadID][0],
											   GroupVertexToTriangleMasks[GroupThreadID][1],
											   GroupVertexToTriangleMasks[GroupThreadID][2],
											   GroupVertexToTriangleMasks[GroupThreadID][3]);
#endif
	}

	return PrimOutput;
}

#endif // NANITE_PRIM_SHADER_CULL

#else // NANITE_PRIM_SHADER

VSOut HWRasterizeVS(
	uint VertexID		: SV_VertexID,
	uint VisibleIndex	: SV_InstanceID
	)
{
#if HAS_PREV_DRAW_DATA
	VisibleIndex += InTotalPrevDrawClusters[0].y;
#endif

#if ADD_CLUSTER_OFFSET
	VisibleIndex += InClusterOffsetSWHW[GetHWClusterCounterIndex(RenderFlags)];
#endif
	VisibleIndex = (MaxClusters - 1) - VisibleIndex;

	uint TriIndex = VertexID / 3;
	VertexID = VertexID - TriIndex * 3;

	VSOut Out;
	Out.Position = float4(0,0,0,1);
	Out.DeviceZ = 0.0f;

	FVisibleCluster VisibleCluster = GetVisibleCluster( VisibleIndex, VIRTUAL_TEXTURE_TARGET );
	FInstanceSceneData InstanceData = GetInstanceData( VisibleCluster.InstanceId );

	FNaniteView NaniteView = GetNaniteView( VisibleCluster.ViewId );
	FTriCluster Cluster = GetCluster(VisibleCluster.PageIndex, VisibleCluster.ClusterIndex);

	if( TriIndex < Cluster.NumTris )
	{
		uint3 TriangleIndices = ReadTriangleIndices( Cluster, TriIndex );
		if( ReverseWindingOrder( InstanceData ) )
		{
			TriangleIndices = uint3( TriangleIndices.x, TriangleIndices.z, TriangleIndices.y );
		}

		uint VertIndex = TriangleIndices[ VertexID ];
		float4 PointClipNoScaling;
		Out = CommonRasterizerVS(NaniteView, InstanceData, VisibleCluster, Cluster, VertIndex, PointClipNoScaling);
	#if PIXEL_VALUE
		Out.PixelValue  = ((VisibleIndex + 1) << 7) | TriIndex;
	#endif
	}

	return Out;
}

#endif // NANITE_PRIM_SHADER

void HWRasterizePS(VSOut In)
{
	uint2 PixelPos = (uint2)In.Position.xy;

	uint PixelValue = 0;
#if PIXEL_VALUE
	PixelValue = In.PixelValue;
#endif

#if VERTEX_TO_TRIANGLE_MASKS
	uint4 Masks0 = LoadParameterCacheP0( In.ToTriangleMasks );
	uint4 Masks1 = LoadParameterCacheP1( In.ToTriangleMasks );
	uint4 Masks2 = LoadParameterCacheP2( In.ToTriangleMasks );

	uint4 Masks = Masks0 & Masks1 & Masks2;
	uint TriangleIndex =	Masks.x ? firstbitlow( Masks.x ) :
							Masks.y ? firstbitlow( Masks.y ) + 32 :
							Masks.z ? firstbitlow( Masks.z ) + 64 :
							firstbitlow( Masks.w ) + 96;

	PixelValue += TriangleIndex;
#endif

#if VIRTUAL_TEXTURE_TARGET
	FNaniteView NaniteView = GetNaniteView(In.ViewId);
#else
	FNaniteView NaniteView;
#endif

#if CLUSTER_PER_PAGE
	PixelPos += In.ViewRect.xy;
	if (all(PixelPos < In.ViewRect.zw))
#elif NANITE_MULTI_VIEW && !NANITE_PRIM_SHADER_CULL
	// In multi-view mode every view has its own scissor, so we have to scissor manually.
	if (all(PixelPos >= In.ViewRect.xy && PixelPos < In.ViewRect.zw))
#endif
	{
		WritePixel(OutVisBuffer64, PixelValue, PixelPos, In.DeviceZ, NaniteView, VIRTUAL_TEXTURE_TARGET);
	#if DEBUG_VISUALIZE
		WritePixel(OutDbgBuffer64, In.DebugValues.x, PixelPos, In.DeviceZ, NaniteView, VIRTUAL_TEXTURE_TARGET);
		InterlockedAdd(OutDbgBuffer32[PixelPos], In.DebugValues.y);
	#endif
	}
}
