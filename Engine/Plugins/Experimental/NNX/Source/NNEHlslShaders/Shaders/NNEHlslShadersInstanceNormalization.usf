// Copyright Epic Games, Inc. All Rights Reserved.

#include "/Engine/Public/Platform.ush"

#define WORK_TYPE float
#define BUFFER_TYPE float
#define READ(x) x
#define WRITE(x) x

int C;							// Number of channels
int NxC;						// Precalculated N x C
int W; 							// D1 x D2 ... Dn

BUFFER_TYPE Epsilon;			// Attribute epsilon to use to avoid division by zero
Buffer<BUFFER_TYPE> Input;		// Input data tensor with dimensions (N x C x D1 x D2 ... Dn)
Buffer<BUFFER_TYPE> Scale;		// Scale tensor of size C
Buffer<BUFFER_TYPE> Bias;		// Bias tensor of size C
RWBuffer<BUFFER_TYPE> Output;	// Output tensor of the same shape as input

#if ALGORITHM == 0
#define GROUP_SIZE_X 1
#define GROUP_SIZE_Y 256
#elif ALGORITHM == 1
#define GROUP_SIZE_X 8
#define GROUP_SIZE_Y 32
#elif ALGORITHM == 2
#define GROUP_SIZE_X 16
#define GROUP_SIZE_Y 16
#elif ALGORITHM == 3
#define GROUP_SIZE_X 32
#define GROUP_SIZE_Y 8
#endif

#if GROUP_SIZE_X > 1

groupshared WORK_TYPE SharedMemory[GROUP_SIZE_X * GROUP_SIZE_Y];

[numthreads(GROUP_SIZE_X, GROUP_SIZE_Y, 1)]
void InstanceNormalization(in const uint3 DispatchThreadID : SV_DispatchThreadID, in const uint3 GroupThreadID : SV_GroupThreadID)
{
	const int StartIdx = DispatchThreadID.y * W;
	const int SharedMemoryOffsetY = GroupThreadID.y * GROUP_SIZE_X;

	const WORK_TYPE InvNum = (WORK_TYPE)1 / (WORK_TYPE)W;

	// Calculate mean
	if (DispatchThreadID.y < NxC)
	{
		WORK_TYPE LocalSum = 0;
		for (int CurrIdx = GroupThreadID.x; CurrIdx < W; CurrIdx += GROUP_SIZE_X)
		{
			LocalSum += READ(Input[StartIdx + CurrIdx]);
		}

		SharedMemory[SharedMemoryOffsetY + GroupThreadID.x] = LocalSum;
	}

	// Wait until all threads wrote result to shared memory
	GroupMemoryBarrierWithGroupSync();

	// Use one thread to calculate final result
	if (GroupThreadID.x == 0 && DispatchThreadID.y < NxC)
	{
		WORK_TYPE LocalMean = 0;

		UNROLL
		for (int Idx = 0; Idx < GROUP_SIZE_X; Idx++)
		{
			LocalMean += SharedMemory[SharedMemoryOffsetY + Idx];
		}

		SharedMemory[SharedMemoryOffsetY] = LocalMean * InvNum;
	}

	// Wait until first thread wrote result to shared memory
	GroupMemoryBarrierWithGroupSync();

	WORK_TYPE Mean = 0;
	if (DispatchThreadID.y < NxC)
	{
		Mean = SharedMemory[SharedMemoryOffsetY];
	
		// Calculate variance
		WORK_TYPE LocalSum = 0;
		WORK_TYPE Tmp = 0;
		for (int CurrIdx = GroupThreadID.x; CurrIdx < W; CurrIdx += GROUP_SIZE_X)
		{
			Tmp = READ(Input[StartIdx + CurrIdx]) - Mean;

			LocalSum += Tmp * Tmp;
		}

		SharedMemory[SharedMemoryOffsetY + GroupThreadID.x] = LocalSum;
	}

	// Wait until all threads wrote result to shared memory
	GroupMemoryBarrierWithGroupSync();

	// Use one thread to calculate final result
	if (GroupThreadID.x == 0 && DispatchThreadID.y < NxC)
	{
		WORK_TYPE LocalVar = 0;

		UNROLL
		for (int Idx = 0; Idx < GROUP_SIZE_X; Idx++)
		{
			LocalVar += SharedMemory[SharedMemoryOffsetY + Idx];
		}

		SharedMemory[SharedMemoryOffsetY] = (WORK_TYPE)1 / sqrt(LocalVar * InvNum + Epsilon);
	}

	// Wait until first thread wrote result to shared memory
	GroupMemoryBarrierWithGroupSync();

	WORK_TYPE Variance = 0;
	if (DispatchThreadID.y < NxC)
	{
		const int ChannelIdx = DispatchThreadID.y % C;
		
		WORK_TYPE InvStdev = SharedMemory[SharedMemoryOffsetY];

		// Carry out instance normalization and write to output
		for (int CurrIdx = GroupThreadID.x; CurrIdx < W; CurrIdx += GROUP_SIZE_X)
		{
			Output[StartIdx + CurrIdx] = WRITE((READ(Input[StartIdx + CurrIdx]) - Mean) * READ(Scale[ChannelIdx]) * InvStdev + READ(Bias[ChannelIdx]));
		}
	}
}

#else

[numthreads(1, 256, 1)]
void InstanceNormalization(in const uint3 DispatchThreadID : SV_DispatchThreadID)
{
	if (DispatchThreadID.y >= NxC || DispatchThreadID.x >= 1)
	{
		return;
	}

	const int StartIdx = DispatchThreadID.y * W;
	const int ChannelIdx = DispatchThreadID.y % C;

	const WORK_TYPE InvNum = (WORK_TYPE)1 / (WORK_TYPE)W;

	// Calculate mean
	WORK_TYPE Mean = 0;
	for (int Idx = 0; Idx < W; Idx++)
	{
		Mean += READ(Input[StartIdx + Idx]);
	}

	Mean *= InvNum;

	// Calculate variance
	WORK_TYPE Variance = 0;
	WORK_TYPE Tmp = 0;
	for (int Idx = 0; Idx < W; Idx++)
	{
		Tmp = READ(Input[StartIdx + Idx]) - Mean;

		Variance += Tmp * Tmp;
	}

	Variance *= InvNum;

	WORK_TYPE InvStdev = (WORK_TYPE)1 / sqrt(Variance + Epsilon);

	// Carry out instance normalization and write to output
	for (int Idx = 0; Idx < W; Idx++)
	{
		Output[StartIdx + Idx] = WRITE((READ(Input[StartIdx + Idx]) - Mean) * READ(Scale[ChannelIdx]) * InvStdev + READ(Bias[ChannelIdx]));
	}
}

#endif